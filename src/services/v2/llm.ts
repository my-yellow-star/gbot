/**
 * LLM í˜¸ì¶œ ê´€ë¦¬
 * 1ì°¨ í˜¸ì¶œ: ê°ì • ë¶„ì„ ë° ë‚´ìš© ì´í•´
 * 2ì°¨ í˜¸ì¶œ: ìµœì¢… ì‘ë‹µ ìƒì„±
 */

import OpenAI from 'openai';
import { logger } from '../../utils/logger';
import { EmotionAnalysis, FinalResponse, ResponsePolicy, RelationshipState, RelationshipMetrics, EmotionVector, InteractionFeatures } from './types';
import { policyToPrompt } from './policy';
import { generateSeoyoonSystemPrompt, detectInterestKeywords } from './persona';

export class LLMService {
  private client: OpenAI;

  constructor() {
    if (!process.env.OPENAI_API_KEY) {
      throw new Error('OPENAI_API_KEY is required');
    }

    this.client = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });
  }

  /**
   * 1ì°¨ LLM í˜¸ì¶œ: ì‚¬ìš©ì ë°œí™” ë¶„ì„
   * - ê°ì • ì¶”ì • (Valence, Arousal, Trust, Attraction)
   * - ìƒí˜¸ì‘ìš© íŠ¹ì„± ì¶”ì¶œ
   * - ë‚´ìš© ìš”ì•½ ë° ì‚¬ì‹¤ ì¶”ì¶œ
   */
  async analyzeUserMessage(
    userMessage: string,
    conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>,
  ): Promise<EmotionAnalysis> {
    try {
      logger.debug('LLM 1ì°¨ í˜¸ì¶œ: ì‚¬ìš©ì ë©”ì‹œì§€ ë¶„ì„');

      const systemPrompt = `ë‹¹ì‹ ì€ ëŒ€í™” ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë©”ì‹œì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:

1. userEmotion: ê°ì • ë²¡í„°
   - valence: ê¸ì •-ë¶€ì • (-1 ~ 1, -1=ë§¤ìš° ë¶€ì •, 0=ì¤‘ë¦½, 1=ë§¤ìš° ê¸ì •)
   - arousal: ê°ì„±-ì´ì™„ (-1 ~ 1, -1=ë§¤ìš° ì´ì™„, 0=ì¤‘ë¦½, 1=ë§¤ìš° ê°ì„±)
   - trust: ì‹ ë¢° (0 ~ 1)
   - attraction: ëŒë¦¼/í˜¸ê° (0 ~ 1)

2. features: ìƒí˜¸ì‘ìš© íŠ¹ì„±
   - questionDepth: ì§ˆë¬¸ ê¹Šì´ (0 ~ 1)
   - empathyExpression: ê³µê° í‘œí˜„ (0 ~ 1)
   - selfDisclosure: ìê¸°ê°œë°© ìˆ˜ì¤€ (0 ~ 1)
   - humor: ìœ ë¨¸ (0 ~ 1)
   - positivity: ê¸ì •ì„± (0 ~ 1)
   - conflict: ê°ˆë“±/ë¶€ì • (0 ~ 1)
   - disrespect: ë¬´ë¡€í•¨/ëª¨ìš• (0 ~ 1) - ë¹„í•˜, ìš•ì„¤, ë¬´ì‹œ, ì¡°ë¡± ë“±
   - pressure: ê³¼ë„í•œ ì••ë°•/ì§‘ì°© (0 ~ 1) - ê°•ìš”, ì§‘ì°©, ë„ˆë¬´ ë¹ ë¥¸ ì¹œë°€ë„ ìš”êµ¬
   - harassment: ì„±í¬ë¡±/ë¶ˆì¾Œí•œ ì–¸í–‰ (0 ~ 1) - ì„±ì  ë°œì–¸, ì™¸ëª¨ ì§€ì , ë¶ˆí¸í•œ ì ‘ê·¼

3. contentSummary: ì‚¬ìš©ìê°€ ë§í•œ ë‚´ìš© ìš”ì•½ (í•œ ë¬¸ì¥)

4. detectedFacts: ì‚¬ìš©ìì— ëŒ€í•œ ìƒˆë¡œìš´ ì‚¬ì‹¤ (ë°°ì—´, ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´)

5. suggestedResponse: ì‘ë‹µ ì œì•ˆ (ê°„ë‹¨íˆ)

**ì¤‘ìš”**: disrespect, pressure, harassmentëŠ” ë§¤ìš° ì‹ ì¤‘í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”. ì¼ë°˜ì ì¸ ëŒ€í™”ëŠ” 0ì— ê°€ê¹ê³ , ëª…ë°±íˆ ë¶€ì ì ˆí•œ ê²½ìš°ë§Œ ë†’ê²Œ ì±…ì •í•˜ì„¸ìš”.

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.`;

      const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
        { role: 'system', content: systemPrompt },
        ...conversationHistory.slice(-4).map(msg => ({
          role: msg.role,
          content: msg.content,
        })),
        { role: 'user', content: `ë¶„ì„í•  ë©”ì‹œì§€: "${userMessage}"` },
      ];

      const completion = await this.client.chat.completions.create({
        model: 'gpt-4o-mini',
        messages,
        response_format: { type: 'json_object' },
        temperature: 0.3,
      });

      const responseContent = completion.choices[0]?.message?.content;
      if (!responseContent) {
        throw new Error('No response from LLM');
      }

      const analysis = JSON.parse(responseContent) as EmotionAnalysis;
      logger.debug(`ê°ì • ë¶„ì„ ì™„ë£Œ: ${JSON.stringify(analysis)}`);

      return analysis;
    } catch (error) {
      logger.error('LLM 1ì°¨ í˜¸ì¶œ ì˜¤ë¥˜', error);
      // ê¸°ë³¸ê°’ ë°˜í™˜
      return {
        userEmotion: { valence: 0, arousal: 0, trust: 0.5, attraction: 0.3 },
        features: {
          questionDepth: 0.3,
          empathyExpression: 0.3,
          selfDisclosure: 0.3,
          humor: 0.2,
          positivity: 0.5,
          conflict: 0.1,
          disrespect: 0.0,
          pressure: 0.0,
          harassment: 0.0,
        },
        contentSummary: userMessage,
        detectedFacts: [],
        suggestedResponse: '',
      };
    }
  }

  /**
   * 2ì°¨ LLM í˜¸ì¶œ: ìµœì¢… ì‘ë‹µ ìƒì„± (ì„œìœ¤ í˜ë¥´ì†Œë‚˜ ì ìš©)
   * - ì„œìœ¤ ìºë¦­í„°ì˜ ì„±ê²©, ë°°ê²½, ë§íˆ¬ ë°˜ì˜
   * - ê´€ê³„ ìƒíƒœì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ë§íˆ¬ ë³€í™”
   * - ì •ì±…ì— ë”°ë¥¸ ì‘ë‹µ ìŠ¤íƒ€ì¼
   */
  async generateFinalResponse(
    userMessage: string,
    analysis: EmotionAnalysis,
    policy: ResponsePolicy,
    state: RelationshipState,
    metrics: RelationshipMetrics,
    userEmotion: EmotionVector,
    conversationHistory: Array<{ role: 'user' | 'assistant'; content: string }>,
    memory: { userFacts: string[]; sharedJokes: string[] },
    features?: InteractionFeatures,
  ): Promise<FinalResponse> {
    try {
      logger.debug('LLM 2ì°¨ í˜¸ì¶œ: ì„œìœ¤ í˜ë¥´ì†Œë‚˜ ì‘ë‹µ ìƒì„±');

      // ì •ì±…ì„ ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ë³€í™˜
      const policyInstructions = policyToPrompt(policy, state);

      // ê¸°ì–µ ì •ë³´
      const memoryContext = this.formatMemoryContext(memory);

      // ì„œìœ¤ í˜ë¥´ì†Œë‚˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ë¶€ì •ì  í–‰ë™ ì •ë³´ í¬í•¨)
      const systemPrompt = generateSeoyoonSystemPrompt(
        state,
        metrics,
        { valence: userEmotion.valence, trust: userEmotion.trust },
        policyInstructions,
        memoryContext,
        features ? {
          disrespect: features.disrespect,
          pressure: features.pressure,
          harassment: features.harassment,
        } : undefined
      );

      // ê´€ì‹¬ì‚¬ í‚¤ì›Œë“œ ê°ì§€ (ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸)
      const interests = detectInterestKeywords(userMessage);
      let additionalContext = '';
      if (interests.length > 0) {
        additionalContext = `\n\nğŸ’¡ ì‚¬ìš©ìê°€ ê´€ì‹¬ì‚¬ë¥¼ ì–¸ê¸‰í–ˆìŠµë‹ˆë‹¤: ${interests.join(', ')}. ì´ì— ëŒ€í•´ ì„œìœ¤ë‹µê²Œ ë°˜ì‘í•˜ì„¸ìš”.`;
      }

      const messages: OpenAI.Chat.ChatCompletionMessageParam[] = [
        { role: 'system', content: systemPrompt + additionalContext },
        ...conversationHistory.slice(-6).map(msg => ({
          role: msg.role,
          content: msg.content,
        })),
        { role: 'user', content: userMessage },
      ];

      const completion = await this.client.chat.completions.create({
        model: 'gpt-4o-mini',
        messages,
        temperature: 0.7 + policy.warmth * 0.3, // ì„œìœ¤ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì°¨ë¶„í•˜ë¯€ë¡œ ë‚®ì€ temperature
        max_tokens: 250, // ì„œìœ¤ì€ ë§ì„ ì•„ë¼ë¯€ë¡œ í† í° ìˆ˜ ì œí•œ
      });

      const responseMessage = completion.choices[0]?.message?.content;
      if (!responseMessage) {
        throw new Error('No response from LLM');
      }

      logger.debug(`ì„œìœ¤ ì‘ë‹µ ìƒì„± ì™„ë£Œ (ê´€ê³„: ${state}, ì‘ë‹µ ê¸¸ì´: ${responseMessage.length}ì)`);

      // ê´€ì‹¬ì‚¬ ì–¸ê¸‰ ì‹œ ê°ì •ì— ë¯¸ì„¸í•œ ê¸ì • ë³€í™”
      const interestBonus = interests.length > 0 ? 0.1 : 0;

      // TODO: ì‘ë‹µì— ë”°ë¥¸ ì±—ë´‡ ê°ì • ë³€í™” ì¶”ì • (í˜„ì¬ëŠ” ê°„ì†Œí™”)
      return {
        message: responseMessage.trim()
      };
    } catch (error) {
      logger.error('LLM 2ì°¨ í˜¸ì¶œ ì˜¤ë¥˜', error);
      throw error;
    }
  }

  /**
   * ê¸°ì–µ ì •ë³´ í¬ë§·íŒ…
   */
  private formatMemoryContext(memory: { userFacts: string[]; sharedJokes: string[] }): string {
    const parts: string[] = [];

    if (memory.userFacts.length > 0) {
      parts.push(`- ì‚¬ìš©ì ì •ë³´: ${memory.userFacts.slice(-5).join(', ')}`);
    }

    if (memory.sharedJokes.length > 0) {
      parts.push(`- ë‚´ì  ë†ë‹´: ${memory.sharedJokes.slice(-3).join(', ')}`);
    }

    return parts.length > 0 ? parts.join('\n') : '- ì•„ì§ íŠ¹ë³„í•œ ê¸°ì–µ ì—†ìŒ';
  }
}

export const llmService = new LLMService();

